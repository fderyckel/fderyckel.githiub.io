<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="François de Ryckel">

  
  
  
    
  
  <meta name="description" content="In the second part of this NLP task, we will use Singular Value Decomposition to help us transform a sparse matrix (from the Document Term Matrix - dtm) into a dense matrix.">

  
  <link rel="alternate" hreflang="en-us" href="https://fderyckel.github.io/post/disaster-tweets-part-ii/">

  


  
  
  
  <meta name="theme-color" content="#EF525B">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cutive+Mono%7CLora:400,700%7CRoboto:400,700&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://fderyckel.github.io/post/disaster-tweets-part-ii/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@Read_The_Dung">
  <meta property="twitter:creator" content="@Read_The_Dung">
  
  <meta property="og:site_name" content="François de Ryckel">
  <meta property="og:url" content="https://fderyckel.github.io/post/disaster-tweets-part-ii/">
  <meta property="og:title" content="Disaster Tweets - Part II | François de Ryckel">
  <meta property="og:description" content="In the second part of this NLP task, we will use Singular Value Decomposition to help us transform a sparse matrix (from the Document Term Matrix - dtm) into a dense matrix."><meta property="og:image" content="https://fderyckel.github.io/post/disaster-tweets-part-ii/featured.jpg">
  <meta property="twitter:image" content="https://fderyckel.github.io/post/disaster-tweets-part-ii/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-05-26T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-06-05T16:04:58&#43;02:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fderyckel.github.io/post/disaster-tweets-part-ii/"
  },
  "headline": "Disaster Tweets - Part II",
  
  "image": [
    "https://fderyckel.github.io/post/disaster-tweets-part-ii/featured.jpg"
  ],
  
  "datePublished": "2020-05-26T00:00:00Z",
  "dateModified": "2020-06-05T16:04:58+02:00",
  
  "author": {
    "@type": "Person",
    "name": "François de Ryckel"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "François de Ryckel",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fderyckel.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "In the second part of this NLP task, we will use Singular Value Decomposition to help us transform a sparse matrix (from the Document Term Matrix - dtm) into a dense matrix."
}
</script>

  

  


  


  





  <title>Disaster Tweets - Part II | François de Ryckel</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">François de Ryckel</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">François de Ryckel</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#hero"><span>Demo</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#featured"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/courses/"><span>Courses</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Disaster Tweets - Part II</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Fri, 05 Jun 2020	
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/disaster-tweets-part-ii/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/r/">R</a></span>
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 709px;">
  <div style="position: relative">
    <img src="/post/disaster-tweets-part-ii/featured_huaca9cf6b5759ed9437f5f07682514be8_1770318_720x0_resize_q90_lanczos.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      


<p>In the second part of this NLP task, we will use Singular Value Decomposition to help us transform a sparse matrix (from the Document Term Matrix - dtm) into a dense matrix. Hence this is still very much a BOW approach. This approach combined with xgboost gave us the best results without using word-embedding (or word-vectors) techniques. That said, we are not sure how this approach would work in production as it seems we would have to constantly regenerate the dense matrix (which is quite computationally intense). We would love to see / hear from others on how to use svd in this type of task.</p>
<p>In a sense, SVD can be seen as a dimensionality reduction technique:going from a very wide sparse matrix (as many columns as there are different words in all the tweets), to a dense one.</p>
<p>So let’s first to build that sparse matrix: on the rows, the document number (in this case the tweet ID) on the columns the word (1 word per column)</p>
<p>Because the dimensionality reduction is based on the words, we need to use the whole dataset for this task. Of course this is not really reasonable in the case of new cases.</p>
<p>Also, since we have already developed a whole cleaning workflow, let’s re-use it on the whole df.</p>
<div id="setting-up" class="section level1">
<h1>Setting up</h1>
<pre class="r"><code>library(readr)      # to read and write (import / export) any type into our R console.
library(dplyr)      # for pretty much all our data wrangling
library(ggplot2)
library(stringr)
library(forcats)
library(purrr)

library(kableExtra)

library(rsample)    # to use initial_split() and some other resampling techniques later on. 
library(recipes)      # to use the recipe() and step_() functions
library(parsnip)      # the main engine that run the models 
library(workflows)    # to use workflow()
library(tune)         # to fine tune the hyperparameters 
library(dials)        # to use grid_regular(), tune_grid(), penalty()
library(yardstick)    # to create the measure of accuracy, f1 score and ROC-AUC 

library(doParallel)   #to parallelize the work - useful  in tune()

library(tidytext)
library(textrecipes)</code></pre>
<p>We’ll be reusing the same clean_tweets() function we have used on part I to clean the tweets. We just copy-paste it here and repurpose it.</p>
<pre class="r"><code>df_train &lt;- read_csv(&quot;~/disaster_tweets/data/train.csv&quot;) %&gt;% as_tibble() %&gt;% select(id, text, keyword, location) 
df_test &lt;- read_csv(&quot;~/disaster_tweets/data/test.csv&quot;) %&gt;% as_tibble() %&gt;% select(id, text, keyword, location)
df_all &lt;- bind_rows(df_train, df_test)

clean_tweets &lt;- function(df){
  df &lt;- df  %&gt;% 
    mutate(number_hashtag = str_count(string = text, pattern = &quot;#&quot;), 
           number_number = str_count(string = text, pattern = &quot;[0-9]&quot;) %&gt;% as.numeric(), 
           number_http = str_count(string = text, pattern = &quot;http&quot;) %&gt;% as.numeric(), 
           number_mention = str_count(string = text, pattern = &quot;@&quot;) %&gt;% as.numeric(), 
           number_location = if_else(!is.na(location), 1, 0), 
           number_keyword = if_else(!is.na(keyword), 1, 0), 
           number_repeated_char = str_count(string = text, pattern = &quot;([a-z])\\1{2}&quot;) %&gt;% as.numeric(),  
           text = str_replace_all(string = text, pattern = &quot;http[^[:space:]]*&quot;, replacement = &quot;&quot;), 
           text = str_replace_all(string = text, pattern = &quot;@[^[:space:]]*&quot;, replacement = &quot;&quot;), 
           number_char = nchar(text),   #add the length of the tweet in character. 
           number_word = str_count(string = text, pattern = &quot;\\w+&quot;), 
           text = str_replace_all(string = text, pattern = &quot;[0-9]&quot;, replacement = &quot;&quot;), 
           text = map(text, textstem::lemmatize_strings) %&gt;% unlist(.), 
           text = map(text, function(.x) stringi::stri_trans_general(.x, &quot;Latin-ASCII&quot;)) %&gt;% unlist(.), 
           text = str_replace_all(string = text, pattern  = &quot;\u0089&quot;, replacement = &quot;&quot;)) %&gt;% 
  select(-keyword, -location) 
  return(df)
}

df_all &lt;- clean_tweets(df_all)</code></pre>
</div>
<div id="finding-the-svd-matrix" class="section level1">
<h1>Finding the SVD matrix</h1>
<p>Let’s now works on our sparse matrix with the bind_tf_idf() functions. First, we’ll need to tokenize the tweets and remove stop-words. To be able to use the tf_idf, we’ll also need to count the occurrence of each word in each tweet.</p>
<pre class="r"><code>df_all_tok &lt;- df_all %&gt;% 
  unnest_tokens(word, text) %&gt;% anti_join(stop_words %&gt;% filter(lexicon == &quot;snowball&quot;)) %&gt;% 
  mutate(word_stem = textstem::stem_words(word)) %&gt;% count(id, word_stem)

df_all_tf_idf &lt;- df_all_tok %&gt;% bind_tf_idf(term = word_stem, document = id, n = n)

# turning the tf_idf into a matrix. 
dtm_df_all &lt;- cast_dtm(term = word_stem, document = id, value = tf_idf, data = df_all_tf_idf)
mat_df_all &lt;- as.matrix(dtm_df_all)
dim(mat_df_all)</code></pre>
<pre><code>## [1] 10873 13802</code></pre>
<pre class="r"><code>length(unique(df_all$id)) </code></pre>
<pre><code>## [1] 10876</code></pre>
<pre class="r"><code># I have a problem! Some tweets have not made it to our matrix.  
# That&#39;s probably because there were just a link, or just a number or just stop words.  
# which one are those links.   This is also why I have hanged the corpus of stop-words. 
# so 3 tweets have not made it at all if we consider both training and testing set. </code></pre>
<p>Let’s have a look at our sparse matrix to better understand what’s going on.</p>
<pre class="r"><code>mat_df_all[1:10, 1:20]</code></pre>
<pre><code>##     Terms
## Docs       car     crash    happen      just  terribl     allah     deed
##    0 0.8580183 0.7837519 0.9588457 0.6432791 1.330996 0.0000000 0.000000
##    1 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.9851632 1.228699
##    2 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##    3 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##    4 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##    5 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##    6 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##    7 0.0000000 0.0000000 0.0000000 0.3216396 0.000000 0.0000000 0.000000
##    8 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##    9 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 0.0000000 0.000000
##     Terms
## Docs earthquak    forgiv       mai    reason         u      citi   differ
##    0 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    1 0.7270493 0.9851632 0.6197444 0.7839108 0.4343156 0.0000000 0.000000
##    2 0.7270493 0.0000000 0.0000000 0.0000000 0.0000000 0.6864859 0.873712
##    3 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    4 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    5 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    6 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    7 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    8 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##    9 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000
##     Terms
## Docs   everyon      hear     safe      stai    across      fire
##    0 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000
##    1 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000
##    2 0.7207918 0.6628682 0.873712 0.7776986 0.0000000 0.0000000
##    3 0.0000000 0.0000000 0.000000 0.0000000 0.6664668 0.3448581
##    4 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.4433889
##    5 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000
##    6 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000
##    7 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000
##    8 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.2586435
##    9 0.0000000 0.0000000 0.000000 0.0000000 0.0000000 0.0000000</code></pre>
<p>The values in the matrix are not the frequency but their tf_idf.</p>
<p>Let’s now fix the issues of the missing tweets or we will have some issues later on during the modeling workflow. We see that the matrix is ordered by ID</p>
<pre class="r"><code># Let&#39;s identify which tweets didn&#39;t make it into our df3 and save them. 
df_mat_rowname &lt;- tibble(id = as.numeric(rownames(mat_df_all)))
df_rowname &lt;- tibble(id = df_all$id)
missing_id &lt;- df_rowname %&gt;% anti_join(df_mat_rowname)

# Let&#39;s add empty rows with the right id as rowname to our matrix. 
yo &lt;- matrix(0.0, nrow = nrow(missing_id), ncol = ncol(mat_df_all))
rownames(yo) &lt;- missing_id$id

mat_df &lt;- rbind(mat_df_all, yo)
dim(mat_df)</code></pre>
<pre><code>## [1] 10876 13802</code></pre>
<pre class="r"><code>#mat_df3[7601:7613, 11290:11302]

### trying to keep track of the order of the matrix
mat_df_id &lt;- rownames(mat_df)
head(mat_df_id, 20)</code></pre>
<pre><code>##  [1] &quot;0&quot;  &quot;1&quot;  &quot;2&quot;  &quot;3&quot;  &quot;4&quot;  &quot;5&quot;  &quot;6&quot;  &quot;7&quot;  &quot;8&quot;  &quot;9&quot;  &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot;
## [16] &quot;15&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot;</code></pre>
<pre class="r"><code>tail(mat_df_id, 20)</code></pre>
<pre><code>##  [1] &quot;10859&quot; &quot;10860&quot; &quot;10861&quot; &quot;10862&quot; &quot;10863&quot; &quot;10864&quot; &quot;10865&quot; &quot;10866&quot; &quot;10867&quot;
## [10] &quot;10868&quot; &quot;10869&quot; &quot;10870&quot; &quot;10871&quot; &quot;10872&quot; &quot;10873&quot; &quot;10874&quot; &quot;10875&quot; &quot;6394&quot; 
## [19] &quot;9697&quot;  &quot;43&quot;</code></pre>
<p>Now that we solved that issue of missing rows (which took almost a all day to figure out), we can move to finding the dense matrix. We will use the <strong>irlba</strong> library to help with the decomposition.</p>
<pre class="r"><code>incomplete.cases &lt;- which(!complete.cases(mat_df))
mat_df[incomplete.cases,] &lt;- rep(0.0, ncol(mat_df))
dim(mat_df) </code></pre>
<pre><code>## [1] 10876 13802</code></pre>
<pre class="r"><code>svd_mat &lt;- irlba::irlba(t(mat_df), nv = 750, maxit = 2000)
write_rds(x = svd_mat, path = &quot;~/disaster_tweets/data/svd.rds&quot;)

# And then to save it the whole df with ID + svd
svd_mat &lt;- read_rds(&quot;~/disaster_tweets/data/svd.rds&quot;)
yo &lt;- as_tibble(svd_mat$v)
dim(yo)</code></pre>
<pre><code>## [1] 10876   750</code></pre>
<pre class="r"><code>df4 &lt;- bind_cols(id = as.numeric(mat_df_id), yo)
write_rds(x = df4, path = &quot;~/disaster_tweets/data/svd_df_all750.rds&quot;)</code></pre>
<p>It is worth mentioning that singular value decomposition didn’t parallelized on my machine and it took a bit over 3hrs to get the matrix. That’s why we have saved it for further used.
[When I used irlba on our university computer (84 cores, over 750 Gb of RAM), it did parallelized very nicely on all core and it didn’t take more than 5 min.]</p>
<p>Now that we have our dense matrix, we can start to fit back all the pieces together for our modelling process.</p>
<pre class="r"><code>df_train &lt;- read_csv(&quot;~/disaster_tweets/data/train.csv&quot;) %&gt;% clean_tweets()

# sorting out the same tweets, different target issues 
temp &lt;- df_train %&gt;% group_by(text) %&gt;% 
  mutate(mean_target = mean(target), 
         new_target = if_else(mean_target &gt; 0.5, 1, 0)) %&gt;% ungroup() %&gt;% 
  mutate(target = new_target, 
         target_bin = factor(if_else(target == 1, &quot;a_truth&quot;, &quot;b_false&quot;))) %&gt;% 
  select(-new_target, -mean_target, -target)


df_svd &lt;- read_rds(&quot;~/disaster_tweets/data/svd_df_all750.rds&quot;)

df_train &lt;- left_join(temp, df_svd, by = &quot;id&quot;) %&gt;% 
  select(-text)</code></pre>
</div>
<div id="svd-with-lasso" class="section level1">
<h1>SVD with Lasso</h1>
<pre class="r"><code>set.seed(0109)
rsplit_df &lt;- initial_split(df_train, strata = target_bin, prop = 0.85)
df_train_tr &lt;- training(rsplit_df)
df_train_te &lt;- testing(rsplit_df)

# reusing the same df_train, df_train_tr, df_train_te from before.  
recipe_tweet &lt;- recipe(formula = target_bin ~ ., data = df_train_tr) %&gt;% 
  update_role(id, new_role = &quot;ID&quot;) %&gt;% 
  step_zv(all_numeric(), -all_outcomes()) %&gt;% 
  step_normalize(all_numeric())

# we &#39;ll assign 40 different values for our penalty. 
# we noticed earlier that best values are between penalties 0.001 and 0.005
grid_lambda &lt;- expand.grid(penalty = seq(0.0014,0.005, length = 45)) 

# This time we&#39;ll use 10 folds cross-validation
set.seed(0109)
folds_training &lt;- vfold_cv(df_train, v = 10, repeats = 1) 

model_lasso &lt;- logistic_reg(mode = &quot;classification&quot;, 
                            penalty = tune(), mixture = 1) %&gt;% 
  set_engine(&quot;glmnet&quot;) 

# starting our worflow
wf_lasso &lt;- workflow() %&gt;% 
  add_recipe(recipe_tweet) %&gt;% 
  add_model(model_lasso) 

library(doParallel)
registerDoParallel(cores = 64)

# run a lasso regression with cross-validation, on 40 different levels of penalty
tune_lasso &lt;- tune_grid(
  wf_lasso, 
  resamples = folds_training, 
  grid = grid_lambda, 
  metrics = metric_set(roc_auc, f_meas, accuracy), 
  control = control_grid(verbose = TRUE)
) 

tune_lasso %&gt;% collect_metrics() %&gt;% 
  write_csv(&quot;~/disaster_tweets/data/metrics_lasso_svd750.csv&quot;)

best_metric &lt;- tune_lasso %&gt;% select_best(&quot;f_meas&quot;)

wf_lasso &lt;- finalize_workflow(wf_lasso, best_metric)

last_fit(wf_lasso, rsplit_df) %&gt;% collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.798
## 2 roc_auc  binary         0.860</code></pre>
<pre class="r"><code>#save the final lasso model
model_lasso_svd &lt;- fit(wf_lasso, df_train)
write_rds(x = model_lasso_svd, path = &quot;~/disaster_tweets/data/model_lasso_svd750.rds&quot;) </code></pre>
<p>Note 1 Lasso: svd with 1000L, normalize all, penalty 0.001681, scores: f1=73.99, acc =79.3, roc=85.4</p>
<div id="analysis-of-grid-results" class="section level2">
<h2>Analysis of grid results</h2>
<pre class="r"><code># we read the results of our sample to see the penalty values and their performances. 
metrics &lt;- read_csv(&quot;~/disaster_tweets/data/metrics_lasso_svd750.csv&quot;) 

metrics %&gt;% 
  ggplot(aes(x = penalty, y = mean, color = .metric)) + 
  geom_line() + 
  facet_wrap(~.metric) + 
  scale_x_log10()</code></pre>
<p><img src="/post/disaster-tweets-II/index_files/figure-html/grid-lasso-1.png" width="672" /></p>
</div>
<div id="make-predictions" class="section level2">
<h2>Make predictions</h2>
<pre class="r"><code>df_test &lt;- read_csv(&quot;~/disaster_tweets/data/test.csv&quot;)  %&gt;% clean_tweets()
df_svd &lt;- read_rds(&quot;~/disaster_tweets/data/svd_df_all750.rds&quot;)
df_test &lt;- left_join(df_test, df_svd, by = &quot;id&quot;) 

library(glmnet)
prediction_lasso_svd &lt;- tibble(id = df_test$id, 
                               target = if_else(predict(model_lasso_svd, new_data = df_test) == &quot;a_truth&quot;, 1, 0))

prediction_lasso_svd %&gt;% write_csv(path = &quot;~/disaster_tweets/data/prediction_svd_lasso750.csv&quot;)

# clean everything 
rm(list =  ls())</code></pre>
<p>On the training set with cross-validation, this model with a penalty of 0.001681, gave us f1 = 73.99, accuracy = 79.3, roc = 85.4. On Kaggle, this model gave us a public score of 76.79. This is not really good considering we got much better results earlier with our <a href="https://fderyckel.github.io/post/disaster-tweets-part-i/#baseline-with-some-additional-features">enhanced approach</a></p>
</div>
</div>
<div id="svd-with-xgboost" class="section level1">
<h1>SVD with Xgboost</h1>
<p>We can use the same idea with xgboost.</p>
<pre class="r"><code>clean_tweets &lt;- function(df){
  df &lt;- df  %&gt;% 
    mutate(number_hashtag = str_count(string = text, pattern = &quot;#&quot;), 
           number_number = str_count(string = text, pattern = &quot;[0-9]&quot;) %&gt;% as.numeric(), 
           number_http = str_count(string = text, pattern = &quot;http&quot;) %&gt;% as.numeric(), 
           number_mention = str_count(string = text, pattern = &quot;@&quot;) %&gt;% as.numeric(), 
           number_location = if_else(!is.na(location), 1, 0), 
           number_keyword = if_else(!is.na(keyword), 1, 0), 
           number_repeated_char = str_count(string = text, pattern = &quot;([a-z])\\1{2}&quot;) %&gt;% as.numeric(),  
           text = str_replace_all(string = text, pattern = &quot;http[^[:space:]]*&quot;, replacement = &quot;&quot;), 
           text = str_replace_all(string = text, pattern = &quot;@[^[:space:]]*&quot;, replacement = &quot;&quot;), 
           number_char = nchar(text),   #add the length of the tweet in character. 
           number_word = str_count(string = text, pattern = &quot;\\w+&quot;), 
           text = str_replace_all(string = text, pattern = &quot;[0-9]&quot;, replacement = &quot;&quot;), 
           text = map(text, textstem::lemmatize_strings) %&gt;% unlist(.), 
           text = map(text, function(.x) stringi::stri_trans_general(.x, &quot;Latin-ASCII&quot;)) %&gt;% unlist(.), 
           text = str_replace_all(string = text, pattern  = &quot;\u0089&quot;, replacement = &quot;&quot;)) %&gt;% 
  select(-keyword, -location) 
  return(df)
}

df_train &lt;- read_csv(&quot;~/disaster_tweets/data/train.csv&quot;) %&gt;% clean_tweets()

# sorting out the same tweets, different target issues 
temp &lt;- df_train %&gt;% group_by(text) %&gt;% 
  mutate(mean_target = mean(target), 
         new_target = if_else(mean_target &gt; 0.5, 1, 0)) %&gt;% ungroup() %&gt;% 
  mutate(target = new_target, 
         target_bin = factor(if_else(target == 1, &quot;a_truth&quot;, &quot;b_false&quot;))) %&gt;% 
  select(-new_target, -mean_target, -target)


df_svd &lt;- read_rds(&quot;~/disaster_tweets/data/svd_df_all750.rds&quot;)

df_train &lt;- left_join(temp, df_svd, by = &quot;id&quot;) %&gt;% 
  select(-text)

recipe_tweet &lt;- recipe(formula = target_bin ~ ., data = df_train) %&gt;% 
  update_role(id, new_role = &quot;ID&quot;)

# xgboost classification, tuning on trees, tree-depth  and mtry
model_xgboost &lt;- boost_tree(mode = &quot;classification&quot;, trees = tune(), 
                            learn_rate = 0.01, tree_depth = tune(), mtry = tune()) %&gt;% 
  set_engine(&quot;xgboost&quot;, nthread = 64)

# starting our workflow
wf_xgboost &lt;- workflow() %&gt;% 
  add_recipe(recipe_tweet) %&gt;% 
  add_model(model_xgboost)

# This time we use 5 folds cross-validation.  
#  xgboost is extremely resource intensive on wide df. 
set.seed(0109)
folds_training &lt;- vfold_cv(df_train, v = 5, repeats = 1)
grid_xgboost &lt;- expand.grid(trees = c(2000), 
                            tree_depth = c(5, 6), 
                            mtry = c(150, 300))

library(doParallel)
registerDoParallel(cores = 64)

# run a xgboost classification with cross-validation
tune_xgboost &lt;- tune_grid(
  wf_xgboost, 
  resamples = folds_training, 
  grid = grid_xgboost, 
  metrics = metric_set(roc_auc, f_meas, accuracy), 
  control = control_grid(verbose = TRUE, save_pred = TRUE)
)

tune_xgboost %&gt;% collect_metrics() %&gt;% 
  write_csv(&quot;~/disaster_tweets/data/metrics_xgboost_svd750.csv&quot;)

best_metric &lt;- tune_xgboost %&gt;% select_best(&quot;f_meas&quot;)

wf_xgboost &lt;- finalize_workflow(wf_xgboost, best_metric)

last_fit(wf_xgboost, rsplit_df) %&gt;% collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.825
## 2 roc_auc  binary         0.883</code></pre>
<pre class="r"><code>#save the final lasso model
model_xgboost_svd &lt;- fit(wf_xgboost, df_train)
write_rds(x = model_xgboost_svd, path = &quot;~/disaster_tweets/data/model_xgboost_svd750.rds&quot;) </code></pre>
<p>Using xgboost in combination with svd gives much better results. Here are a few things that we have tried with our training data:</p>
<ul>
<li>svd 1000 wide matrix and xgboost with 150 mtry, 2500 trees, 5 tree-depth, gave us f1 = 74.77, accuracy = 80.90, roc = 86.45<br />
</li>
<li>svd 750 wide matrix and xgboost with 150 mtry, 2000 trees, 6 tree-depth, gave us f1 = 74.99, accuracy = 81.05, roc = 87</li>
<li>svd 500 wide matrix and xgboost with 200 mtry, 2000 trees, 6 tree-depth, gave us f1 = 75.11, accuracy = 81.02, roc = 86.87</li>
<li>svd 250 wide matrix and xgboost with 125 mtry, 1500 trees, 5 tree-depth, gave us f1 = 74.93, accuracy = 80.81, roc = 86.62</li>
</ul>
<div id="variable-importance" class="section level2">
<h2>variable importance</h2>
<pre class="r"><code>library(vip)
model_xgboost_svd %&gt;% 
  pull_workflow_fit() %&gt;% 
  vip::vip(geom = &quot;point&quot;, num_features=20) #%&gt;% arrange(desc(Importance)) %&gt;% </code></pre>
<p><img src="/post/disaster-tweets-II/index_files/figure-html/vip-1.png" width="672" /></p>
<p>Clearly, we can’t interpret anymore our variables as they are the result of singular variable decomposition of a tf-idf sparse matrix. However, we are happy to see that our extra variables have played a role in determining if a tweet was about real disaster or not.</p>
</div>
</div>
<div id="submission-of-results" class="section level1">
<h1>Submission of results</h1>
<pre class="r"><code>df_test &lt;- read_csv(&quot;~/disaster_tweets/data/test.csv&quot;)  %&gt;% clean_tweets()
df_svd &lt;- read_rds(&quot;~/disaster_tweets/data/svd_df_all750.rds&quot;)
df_test &lt;- left_join(df_test, df_svd, by = &quot;id&quot;) 

library(xgboost)
prediction_xgboost_svd &lt;- tibble(id = df_test$id, 
                                 target = if_else(predict(model_xgboost_svd, new_data = df_test) == &quot;a_truth&quot;, 1, 0))

prediction_xgboost_svd %&gt;% write_csv(path = &quot;~/disaster_tweets/data/prediction_svd_xgboost750.csv&quot;)</code></pre>
<p>Note 1: majority voting, svd with 850 wide, using lasso, got 77% public score.</p>
<p>Note 2: majority voting, svd 500 wide, using xgboost with 200 mtry, 2000 trees, 6 tree-depth, got a 80.01 public score.</p>
<p>Note 3: majority voting, svd with 750 wide, using xgboost with 200 mtry, 2000 trees, 6 tree-depth, got 81.29% public score. Yeahhh!!!!!!!</p>
<p>Here is a screenshot of our results:<br />
<img src="/img/screenshot-results.png" alt="screenshot of results" /></p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li>To help with the use of irlba and <a href="https://www.kaggle.com/barun2104/nlp-with-disaster-eda-dfm-svd-ensemble">check for the complete matrix</a></li>
</ul>
</div>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/classification/">Classification</a>
  
  <a class="badge badge-light" href="/tag/kaggle/">kaggle</a>
  
  <a class="badge badge-light" href="/tag/svd/">SVD</a>
  
  <a class="badge badge-light" href="/tag/tidymodels/">tidymodels</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://fderyckel.github.io/post/disaster-tweets-part-ii/&amp;text=Disaster%20Tweets%20-%20Part%20II" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://fderyckel.github.io/post/disaster-tweets-part-ii/&amp;t=Disaster%20Tweets%20-%20Part%20II" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Disaster%20Tweets%20-%20Part%20II&amp;body=https://fderyckel.github.io/post/disaster-tweets-part-ii/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://fderyckel.github.io/post/disaster-tweets-part-ii/&amp;title=Disaster%20Tweets%20-%20Part%20II" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Disaster%20Tweets%20-%20Part%20II%20https://fderyckel.github.io/post/disaster-tweets-part-ii/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://fderyckel.github.io/post/disaster-tweets-part-ii/&amp;title=Disaster%20Tweets%20-%20Part%20II" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/francois-de-ryckel/avatar_hu2f41e612dcff73a93739a326eb68bc0c_287175_270x270_fill_lanczos_center_2.png" alt="François de Ryckel">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://fderyckel.github.io/">François de Ryckel</a></h5>
        <h6 class="card-subtitle">Teacher, Farmer, Educational Data Analyst</h6>
        <p class="card-text">A travelling educator with a strong interest in farming, finance and modeling.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/Read_The_Dung" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/francois-de-ryckel/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/fderyckel" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  







<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "fderyckelgithubio" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>










  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    <script id="dsq-count-scr" src="https://fderyckelgithubio.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.37431be2d92d7fb0160054761ab79602.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
